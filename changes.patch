diff --git a/run.py b/run.py
index cf8afc8..8a4dd50 100644
--- a/run.py
+++ b/run.py
@@ -13,7 +13,7 @@ import sys
 import tempfile
 from typing import List
 
-from tritonbench.operator_loader import load_opbench_by_name_from_loader
+# from tritonbench.operator_loader import load_opbench_by_name_from_loader
 from tritonbench.operators import load_opbench_by_name
 from tritonbench.operators_collection import list_operators_by_collection
 from tritonbench.utils.gpu_utils import gpu_lockdown
diff --git a/tritonbench/kernels/triton_fused_attention.py b/tritonbench/kernels/triton_fused_attention.py
index d418039..5eb0a06 100644
--- a/tritonbench/kernels/triton_fused_attention.py
+++ b/tritonbench/kernels/triton_fused_attention.py
@@ -20,11 +20,15 @@ import torch
 import triton
 import triton.language as tl
 
+import triton.profiler as proton
+
 # check if we have the TMA version in Triton PR #4498 (https://github.com/triton-lang/triton/pull/4498).
 HAS_TMA_DESC = "nv_tma_desc_type" in dir(tl)
 WITH_COMPPIPE = os.getenv("ENABLE_COMPPIPE")
 PEEL_LAST = os.getenv("PEEL_LAST_ITER")
 
+NUM_SLOTS=384
+
 if HAS_TMA_DESC:
     print(
         "TMA benchmarks will be running with experimental grid constant TMA descriptor.",
@@ -155,7 +159,7 @@ def _attn_fwd_inner(
         K_block_ptr = tl.advance(K_block_ptr, (0, lo))
         V_block_ptr = tl.advance(V_block_ptr, (lo, 0))
     # loop over k, v and update accumulator
-    for start_n in tl.range(lo, hi, BLOCK_N): #, loop_schedule=LOOP_SCHEDULE):
+    for start_n in tl.range(lo, hi, BLOCK_N, loop_schedule=LOOP_SCHEDULE):
         start_n = tl.multiple_of(start_n, BLOCK_N)
         # -- compute qk ----
         if ENABLE_TMA:
@@ -259,7 +263,7 @@ def _attn_fwd_inner_ws(
         K_block_ptr = tl.advance(K_block_ptr, (0, lo))
         V_block_ptr = tl.advance(V_block_ptr, (lo, 0))
     # loop over k, v and update accumulator
-    for start_n in tl.range(lo, hi, BLOCK_N): #, loop_schedule=LOOP_SCHEDULE):
+    for start_n in tl.range(lo, hi, BLOCK_N, loop_schedule=LOOP_SCHEDULE):
         start_n = tl.multiple_of(start_n, BLOCK_N)
         # -- compute qk ----
         with tl.async_task([0]):
@@ -328,11 +332,18 @@ def _attn_fwd_inner_ws(
 
 # We don't run auto-tuning every time to keep the tutorial fast. Uncommenting
 # the code below and commenting out the equivalent parameters is convenient for
-# re-tuning.
+# re-tuningtri
 has_warp_spec = hasattr(tl, "async_task")
-schedList = ["default", "FA_firstDot", "FA_secondDot"] if WITH_COMPPIPE else ["default"]
+# print('has warp spec: %s' % (has_warp_spec))
+schedList = ["FA_secondDot"] if WITH_COMPPIPE else ["default"]
 # TODO: incorrect result with PEEL_LAST + FA_firstDot + WarpSpec + TMA
-schedList = ["FA_secondDot"] if PEEL_LAST else schedList
+# schedList = ["FA_secondDot"] if PEEL_LAST else schedList
+# schedList = ["default"]
+# schedList = ["FA_firstDot"]
+# schedList = ["FA_secondDot"]
+# PEEL_LAST = False
+# print('PEEL_LAST: %s' %(PEEL_LAST))
+# print('schedlist: %s' %(schedList))
 # no WS, no TMA, with CompPipe
 configsOpt = [
     (
@@ -441,6 +452,7 @@ configsOrig = [
             num_warps=w,
             num_buffers_warp_spec=0,
             num_consumer_groups=0,
+            proton_slots=NUM_SLOTS,
         )
         if has_warp_spec
         else triton.Config(
@@ -456,7 +468,7 @@ configsOrig = [
     )
     for BM in [128]  # 64, 128]
     for BN in [128]  # 64, 128]
-    for s in [3]  # , 4, 7]
+    for s in [2]  # , 4, 7]
     for w in [8]  # 4, 8]
 ]
 # TMA, WS, and CompPipe
@@ -475,6 +487,7 @@ configsTmaWS = [
             num_consumer_groups=grp,
             reg_dec_producer=dec,
             reg_inc_consumer=inc,
+            proton_slots=NUM_SLOTS,
         )
         if has_warp_spec
         else triton.Config(
@@ -910,8 +923,7 @@ def _attn_fwd_ws(
     BLOCK_N: tl.constexpr,  #
     HEAD_DIM: tl.constexpr,  #
     STAGE: tl.constexpr,  #
-    ENABLE_TMA: tl.constexpr,
-    LOOP_SCHEDULE: tl.constexpr,
+    ENABLE_TMA: tl.constexpr,    LOOP_SCHEDULE: tl.constexpr,
     ENABLE_WS: tl.constexpr,
 ):
     _attn_fwd_compute_ws(
@@ -985,6 +997,7 @@ def _attn_fwd(
     Z,
     H,
     N_CTX,  #: tl.constexpr,  #
+    profile_mem,
     BLOCK_M: tl.constexpr,  #
     BLOCK_N: tl.constexpr,  #
     HEAD_DIM: tl.constexpr,  #
@@ -1143,6 +1156,7 @@ def _attn_fwd_tma(  # Q, V, desc_k, desc_v, sm_scale, M, Out,  #
     Z,
     H,
     N_CTX,  #: tl.constexpr,  #
+    profile_mem,
     BLOCK_M: tl.constexpr,  #
     BLOCK_N: tl.constexpr,  #
     HEAD_DIM: tl.constexpr,  #
@@ -1222,6 +1236,7 @@ def _attn_fwd_tma_ws(  # Q, V, desc_k, desc_v, sm_scale, M, Out,  #
     Z,
     H,
     N_CTX,  #: tl.constexpr,  #
+    profile_mem,
     BLOCK_M: tl.constexpr,  #
     BLOCK_N: tl.constexpr,  #
     HEAD_DIM: tl.constexpr,  #
@@ -1611,6 +1626,7 @@ def _attn_bwd(
 class _attention_opt(torch.autograd.Function):
     @staticmethod
     def forward(ctx, q, k, v, causal, sm_scale, baseVariant):
+        # print('has warp spec: %s' % (has_warp_spec))
         # shape constraints
         HEAD_DIM_Q, HEAD_DIM_K = q.shape[-1], k.shape[-1]
         # when v is in float8_e5m2 it is transposed.
@@ -1632,6 +1648,13 @@ class _attention_opt(torch.autograd.Function):
 
         def grid_tma(META):
             if META["ENABLE_TMA"] == False:
+                # print((
+                #     # grid partitioning: num_consumer_groups * BLOCK_M
+                #     # data partitioning: BLOCK_M
+                #     triton.cdiv(q.shape[2], META["BLOCK_M"]),  # num_consumer_groups
+                #     q.shape[0] * q.shape[1],
+                #     1,
+                # ))
                 return (
                     # grid partitioning: num_consumer_groups * BLOCK_M
                     # data partitioning: BLOCK_M
@@ -1706,6 +1729,16 @@ class _attention_opt(torch.autograd.Function):
             (q.shape[0], q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32
         )
         if baseVariant == "base":
+            manual_meta = {
+                'ENABLE_TMA': False,
+                'ENABLE_WS' : False,
+                'BLOCK_M' : 128,
+                'BLOCK_N' : 128,
+            }
+            manual_grid = grid_tma(manual_meta)
+            pconfig = proton.IntraKernelConfig(slots=NUM_SLOTS, header=3, wg_num=2, word_per_slot=2)
+            scratch = proton.intra_kernel_smem(pconfig)
+            profile_mem = torch.empty((np.prod(manual_grid) * scratch), device="cuda", dtype=torch.uint32)
             _attn_fwd[grid_tma](
                 q,
                 k,
@@ -1735,12 +1768,16 @@ class _attention_opt(torch.autograd.Function):
                 o.stride(3),  #
                 q.shape[0],
                 q.shape[1],  #
-                N_CTX=q.shape[2],  #
+                q.shape[2],  #
+                profile_mem,
                 HEAD_DIM=HEAD_DIM_K,  #
                 STAGE=stage,  #
                 ENABLE_WS=False,
                 **extra_kern_args,
             )
+            enable_profiling = os.getenv("TRITON_KERNEL_OVERRIDE", "0") == "1"
+            if enable_profiling:
+                proton.dump_chrome_trace(32, pconfig, profile_mem, "chrome_trace.json")
         elif baseVariant == "ws":
             _attn_fwd_ws[grid_tma](
                 q,
@@ -1814,6 +1851,16 @@ class _attention_opt(torch.autograd.Function):
                 **extra_kern_args,
             )
         elif baseVariant == "tma":
+            manual_meta = {
+                'ENABLE_TMA': True,
+                'ENABLE_WS' : True,
+                'BLOCK_M' : 64,
+                'BLOCK_N' : 128,
+            }
+            manual_grid = grid_tma(manual_meta)
+            pconfig = proton.IntraKernelConfig(slots=NUM_SLOTS, header=3, wg_num=3, word_per_slot=2)
+            scratch = proton.intra_kernel_smem(pconfig)
+            profile_mem = torch.empty((np.prod(manual_grid) * scratch), device="cuda", dtype=torch.uint32)
             _attn_fwd_tma[grid_tma](
                 q,
                 k,
@@ -1843,48 +1890,105 @@ class _attention_opt(torch.autograd.Function):
                 o.stride(3),  #
                 q.shape[0],
                 q.shape[1],  #
-                N_CTX=q.shape[2],  #
+                q.shape[2],  #
+                profile_mem,
                 HEAD_DIM=HEAD_DIM_K,  #
                 STAGE=stage,  #
                 ENABLE_WS=False,
                 **extra_kern_args,
             )
+            enable_profiling = os.getenv("TRITON_KERNEL_OVERRIDE", "0") == "1"
+            if enable_profiling:
+                proton.dump_chrome_trace(32, pconfig, profile_mem, "chrome_trace.json")
         elif baseVariant == "tma_ws":
-            _attn_fwd_tma_ws[grid_tma](
-                q,
-                k,
-                v,
-                sm_scale,
-                M,
-                o,
-                desc_q,
-                desc_k,
-                desc_v,
-                desc_o,  #
-                q.stride(0),
-                q.stride(1),
-                q.stride(2),
-                q.stride(3),  #
-                k.stride(0),
-                k.stride(1),
-                k.stride(2),
-                k.stride(3),  #
-                v.stride(0),
-                v.stride(1),
-                v.stride(2),
-                v.stride(3),  #
-                o.stride(0),
-                o.stride(1),
-                o.stride(2),
-                o.stride(3),  #
-                q.shape[0],
-                q.shape[1],  #
-                N_CTX=q.shape[2],  #
-                HEAD_DIM=HEAD_DIM_K,  #
-                STAGE=stage,  #
-                ENABLE_WS=True,
-                **extra_kern_args,
-            )
+            # enable_profiling = True
+            enable_profiling = os.getenv("TRITON_KERNEL_OVERRIDE", "0") == "1"
+            # from configsTmaWS
+            manual_meta = {
+                'ENABLE_TMA': True,
+                'ENABLE_WS' : True,
+                'BLOCK_M' : 128,
+                'BLOCK_N' : 128,
+            }
+            manual_grid = grid_tma(manual_meta)
+            pconfig = proton.IntraKernelConfig(slots=NUM_SLOTS, header=3, wg_num=3, word_per_slot=2)
+            scratch = proton.intra_kernel_smem(pconfig)
+            profile_mem = torch.empty((np.prod(manual_grid) * scratch), device="cuda", dtype=torch.uint32)
+            if  enable_profiling:
+                _attn_fwd_tma_ws[grid_tma](
+                    q,
+                    k,
+                    v,
+                    sm_scale,
+                    M,
+                    o,
+                    desc_q,
+                    desc_k,
+                    desc_v,
+                    desc_o,  
+                    q.stride(0),
+                    q.stride(1),
+                    q.stride(2),
+                    q.stride(3),  #
+                    k.stride(0),
+                    k.stride(1),
+                    k.stride(2),
+                    k.stride(3),  #
+                    v.stride(0),
+                    v.stride(1),
+                    v.stride(2),
+                    v.stride(3),  #
+                    o.stride(0),
+                    o.stride(1),
+                    o.stride(2),
+                    o.stride(3),  #
+                    q.shape[0],
+                    q.shape[1],  
+                    q.shape[2],  
+                    profile_mem,
+                    HEAD_DIM=HEAD_DIM_K,  #
+                    STAGE=stage,  #
+                    ENABLE_WS=True,#
+                    **extra_kern_args,
+                )
+                proton.dump_chrome_trace(32, pconfig, profile_mem, "chrome_trace.json")
+            else:
+                _attn_fwd_tma_ws[grid_tma](
+                    q,
+                    k,
+                    v,
+                    sm_scale,
+                    M,
+                    o,
+                    desc_q,
+                    desc_k,
+                    desc_v,
+                    desc_o,  
+                    q.stride(0),
+                    q.stride(1),
+                    q.stride(2),
+                    q.stride(3),  #
+                    k.stride(0),
+                    k.stride(1),
+                    k.stride(2),
+                    k.stride(3),  #
+                    v.stride(0),
+                    v.stride(1),
+                    v.stride(2),
+                    v.stride(3),  #
+                    o.stride(0),
+                    o.stride(1),
+                    o.stride(2),
+                    o.stride(3),  #
+                    q.shape[0],
+                    q.shape[1],  
+                    q.shape[2],  
+                    profile_mem,
+                    HEAD_DIM=HEAD_DIM_K,  #
+                    STAGE=stage,  #
+                    ENABLE_WS=True,#
+                    **extra_kern_args,
+                )
 
         ctx.save_for_backward(q, k, v, o, M)
         ctx.grid = grid_tma
